import json
import sys
sys.path.append(".")
from cs336_basics.pretokenization_example import find_chunk_boundaries
import regex as re
import heapq
from collections import defaultdict
from typing import Iterable,Iterator

# regex based pre-tokenizer
PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
NUM_CHUNKS = 4


def merge(byte_text:str,new_idx:int,pair:tuple[bytes,bytes])->list[int]:
    doc_encode = []
    i = 0
    while i< len(byte_text):
        if i + 1 < len(byte_text) and byte_text[i] == pair[0] and byte_text[i + 1] == pair[1]:
            doc_encode.append(new_idx)
            i += 2
        else:
            doc_encode.append(byte_text[i])
            i += 1
    return doc_encode

class Tokenizer:
    """Abstract inferface of tokenizer """
    def encode(self, string: str) -> list[int]:
        raise NotImplementedError
    def decode(self, indices: list[int]) -> str:
        raise NotImplementedError

class BPETokenizer(Tokenizer):
    def __init__(self,vocab:dict[int,bytes],merges:list[tuple[bytes,bytes]],special_tokens:list[str]|None=None):
        self.vocab = vocab
        self.merges = merges
        self.special_tokens = special_tokens or []
        self.inverted_vocab = {v: k for k, v in vocab.items()}
        # é¢„è®¡ç®—å­—èŠ‚å½¢å¼çš„ç‰¹æ®Štoken
        self.byte_special_tokens = [token.encode('utf-8') for token in self.special_tokens]

    @classmethod
    def from_files(cls,vocab_filepath,merges_filepath,special_tokens=None):
        ## ä»æ–‡ä»¶åŠ è½½vocabå’Œmerges, è¿”å›å®ä¾‹ ##
        vocab:dict[int,bytes] = {}
        with open(vocab_filepath,'r',encoding='utf-8') as f:
            content = json.load(f)
        for key,value in content.items():
            vocab[value] = key.encode("utf-8",errors="ignore")
        del content
        merges:list[tuple[bytes,bytes]] = []
        with open(merges_filepath,"rb") as f:
            for line in f:
                line = line.strip().split()
                merges.append((line[0],line[1]))
        
        return cls(vocab,merges,special_tokens)
    
    def encode(self,text:str)->list[int]:
        # pre-tokenize text
        if self.special_tokens == []:
            parts = [text]
        else:
            sorted_special_tokens = sorted(self.special_tokens,key=lambda x: -len(x)) # æ›´é•¿çš„special tokenä¼šæ’åœ¨å‰é¢
            delimiter_pattern = "|".join(re.escape(token) for token in sorted_special_tokens)
            parts = re.split('(' + delimiter_pattern + ')',text) # partå†…å°±èƒ½åŒ…æ‹¬special tokens
        byte_pretokens = []
        for part in parts:
            if part in self.special_tokens:
                spec_tok_bytes = part.encode('utf-8')
                byte_pretokens.extend([spec_tok_bytes]) # å¦‚æœæ˜¯special tokensåˆ™åŠ å…¥special tokens
            else:
                str_tokens = re.findall(PAT, part)
                part_tokens = [s.encode('utf-8') for s in str_tokens] # å¦‚æœæ²¡æœ‰åˆ™æ­£å¸¸decode
                byte_pretokens.extend(part_tokens)
    
        pretokens = []  # list[list[int]]

        # Convert pretokens from bytes to list[int] by vocab
        for pretoken in byte_pretokens:
            new_pretoken = []
            if pretoken in self.byte_special_tokens:
                index = self.inverted_vocab[pretoken]
                new_pretoken.append(index)
            else:
                for b in pretoken:
                    #index = inverted_vocab[bytes([b])]
                    index = self.inverted_vocab.get(bytes([b]),0)
                    new_pretoken.append(index)

            pretokens.append(new_pretoken)
            #pretokens.extend(new_pretoken)
        
        #merge:
        # for i,pretoken in enumerate(pretokens):
        #     for pair in self.merges:
        #         new_idx = self.inverted_vocab[pair[0] + pair[1]]
        #         if new_idx == 15137:
        #             print("hey!")
        #         new_token = []
        #         j = 0
        #         while j< len(pretoken):
        #             if j + 1 < len(pretoken) and ((self.vocab[pretoken[j]] , self.vocab[pretoken[j + 1]]) == pair):
        #             #if j + 1 < len(pretoken) and ((self.vocab[pretoken[j]] , self.vocab[pretoken[j + 1]]) == pair):
        #                 new_token.append(new_idx)
        #                 j += 2
        #             else:
        #                 new_token.append(pretoken[j])
        #                 j += 1
        #         # if new_token[0] == 15137:
        #         #     print("hey!")
        #         pretoken = new_token
        #     pretokens[i] = pretoken
        for i,pretoken in enumerate(pretokens):
            pretokens[i] = self._merge_fast(pretoken,self.inverted_vocab)
        return [token for pretoken in pretokens for token in pretoken]
        
        

        # # convert pretokens from byte to list[int]
        # for pretoken in pretokens_byte:
        #     new_pretoken = []

        #     if pretoken in byte_special_tokens:
        #         index = self.vocab[pretoken]
        #         new_pretoken.append(index)
        #     else:
        #         for b in pretoken:
        #             index = self.vocab[bytes([b])]
        #             new_pretoken.append(index)

        #     pretokens.extend(new_pretoken)
        # return self._merge_fast(pretokens)


    def encode_iterable(self,iterable:Iterator[str])->Iterator[int]:
        # # å†™æ³•1
        # for line in iterable:
        #     for idx in self.encode(line):
        #         yield idx
        for line in iterable:
            yield from self.encode(line)
    
    def decode(self,ids:list[int]) -> str:
        # inverted_vocab = dict(zip(self.vocab.values(),self.vocab.keys()))
        # byte_sequences = []

        # for token_id in ids:
        #     byte_seq = inverted_vocab.get(token_id, b'')  # å¤„ç†æœªçŸ¥ID
        #     byte_sequences.append(byte_seq)
        
        # all_bytes = b''.join(byte_sequences)
        
        # # ä½¿ç”¨errors='replace'è‡ªåŠ¨æ›¿æ¢æ— æ•ˆå­—èŠ‚
        # text = all_bytes.decode('utf-8', errors='replace')
        # return text
        
        tokens = b''
        vocab_size = len(self.vocab)
        replacement_char = "\uFFFD"
        for id in ids:
            if id > vocab_size:
                tokens += bytes(replacement_char,encoding="utf-8")
            else:
                tokens += self.vocab[id]
        return tokens.decode("utf-8",errors="replace")
        
    def _merge_fast(self,tokens:list[bytes],inverted_vocab:dict[bytes,int])->list[int]:
        # build merge map: merge pair to id
        merge_map:dict[tuple[bytes,bytes],int] = {}
        for pair in self.merges:
            merge_map[pair] = inverted_vocab[pair[0] + pair[1]]
        # all possible merged positions
        positions = defaultdict(list)
        for i in range(len(tokens)-1):
            pair = (self.vocab[tokens[i]],self.vocab[tokens[i+1]])
            if pair in merge_map:
                heapq.heappush(positions[pair],i)
        
        # apply merge to positions, ä»æœ€å…ˆå‡ºç°çš„pairå¼€å§‹åˆå¹¶
        while positions:
            best_pair = None
            best_pos = float('inf') # æ•´ä¸ªæ–‡æ®µä¸­ç¬¬ä¸€ä¸ªå‡ºç°merged pairçš„ä½ç½®
            for pair,pos_list in positions.items():
                # å¦‚æœå­˜åœ¨pos_listï¼Œå³pairæ˜¯å¯ä»¥è¢«åˆå¹¶çš„ï¼Œå°±æ‰¾è¿™ä¸ªpairæœ€å…ˆå‡ºç°çš„ä½ç½®
                if pos_list and pos_list[0] < best_pos:
                    best_pos = pos_list[0]
                    best_pair = pair
            if best_pair is None:
                break

            # apply merge to token
            tokens[best_pos] = merge_map[best_pair]
            tokens.pop(best_pos+1)# å»æ‰å½“å‰tokençš„åä¸€ä¸ªtoken

            # æ›´æ–°å—å½±å“çš„ä½ç½®
            for pair in list(positions.keys()):
                new_positions = []
                for pos in positions[pair]: # æ¯ä¸€ä¸ªpairå¯¹åº”çš„positionåˆ—è¡¨
                    if pos==best_pos or pos==best_pos+1: # åˆ é™¤å½“å‰pair
                        continue
                    elif pos > best_pos+1:
                        new_positions.append(pos-1) # æ‰€æœ‰çš„pairä½ç½®å¾€å‰ç§»ä¸€ä½
                    else:
                        new_positions.append(pos)
                if new_positions:
                    positions[pair] = new_positions
                    heapq.heapify(positions[pair])
                else:
                    del positions[pair] # if pos==best_pos or pos==best_pos+1å‰©ä¸‹çš„åœ¨è¿™é‡Œåˆ é™¤
            # æ£€æŸ¥æ–°åˆ›å»ºçš„åˆå¹¶å¯¹
            # å·¦ä¾§é‚»å±…å¯¹
            if best_pos>0:
                left_pair = (self.vocab[tokens[best_pos-1]],self.vocab[tokens[best_pos]])
                if left_pair in merge_map:
                    heapq.heappush(positions[left_pair],best_pos-1)
            # å³ä¾§é‚»å±…å¯¹
            if best_pos < len(tokens)-1:
                right_pair = (self.vocab[tokens[best_pos]],self.vocab[tokens[best_pos+1]])
                if right_pair in merge_map:
                    heapq.heappush(positions[right_pair],best_pos)
        return tokens
        
            


if __name__ == "__main__":
    ## test code ##
    tokenizer = BPETokenizer.from_files(
        vocab_filepath="tests/fixtures/gpt2_vocab.json",
        merges_filepath="tests/fixtures/gpt2_merges.txt",
    )
    # print(tokenizer.vocab)
    # print(tokenizer.merges[:100]) 

    with open("tests/fixtures/address.txt") as f:
        corpus_contents = f.read()
    #test_string = "HÃ©llÃ² hÃ´w <|endoftext|><|endoftext|> are Ã¼? ğŸ™ƒ<|endoftext|>"
    ids = tokenizer.encode(corpus_contents)
    print(ids)
    print(tokenizer.decode(ids))
    tokenized_string = [tokenizer.decode([x]) for x in ids]
    print(tokenized_string)



        